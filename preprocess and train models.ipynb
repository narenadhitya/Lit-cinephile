{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data and training models...\n",
      "Loading datasets...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning overview: 100%|██████████| 46632/46632 [00:15<00:00, 3082.20it/s]\n",
      "Cleaning keywords: 100%|██████████| 46632/46632 [00:05<00:00, 8785.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "Encoding movie features with DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding texts: 100%|██████████| 1458/1458 [07:17<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NCF model...\n",
      "Epoch 1/25, Train Loss: 0.9687, Val Loss: 0.7480\n",
      "Epoch 2/25, Train Loss: 0.7381, Val Loss: 0.7171\n",
      "Epoch 3/25, Train Loss: 0.7136, Val Loss: 0.7009\n",
      "Epoch 4/25, Train Loss: 0.7004, Val Loss: 0.6928\n",
      "Epoch 5/25, Train Loss: 0.6941, Val Loss: 0.6879\n",
      "Epoch 6/25, Train Loss: 0.6904, Val Loss: 0.6869\n",
      "Epoch 7/25, Train Loss: 0.6881, Val Loss: 0.6839\n",
      "Epoch 8/25, Train Loss: 0.6872, Val Loss: 0.6843\n",
      "Epoch 9/25, Train Loss: 0.6862, Val Loss: 0.6823\n",
      "Epoch 10/25, Train Loss: 0.6857, Val Loss: 0.6842\n",
      "Epoch 11/25, Train Loss: 0.6850, Val Loss: 0.6812\n",
      "Epoch 12/25, Train Loss: 0.6843, Val Loss: 0.6807\n",
      "Epoch 13/25, Train Loss: 0.6838, Val Loss: 0.6800\n",
      "Epoch 14/25, Train Loss: 0.6834, Val Loss: 0.6804\n",
      "Epoch 15/25, Train Loss: 0.6829, Val Loss: 0.6792\n",
      "Epoch 16/25, Train Loss: 0.6827, Val Loss: 0.6791\n",
      "Epoch 17/25, Train Loss: 0.6824, Val Loss: 0.6795\n",
      "Epoch 18/25, Train Loss: 0.6820, Val Loss: 0.6793\n",
      "Epoch 19/25, Train Loss: 0.6817, Val Loss: 0.6788\n",
      "Epoch 20/25, Train Loss: 0.6812, Val Loss: 0.6776\n",
      "Epoch 21/25, Train Loss: 0.6809, Val Loss: 0.6803\n",
      "Epoch 22/25, Train Loss: 0.6804, Val Loss: 0.6773\n",
      "Epoch 23/25, Train Loss: 0.6801, Val Loss: 0.6787\n",
      "Epoch 24/25, Train Loss: 0.6799, Val Loss: 0.6773\n",
      "Epoch 25/25, Train Loss: 0.6796, Val Loss: 0.6774\n",
      "Training LightGBM model...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 0.0755832\n",
      "[200]\tvalid_0's rmse: 0.0293739\n",
      "[300]\tvalid_0's rmse: 0.0123672\n",
      "[400]\tvalid_0's rmse: 0.00568223\n",
      "[500]\tvalid_0's rmse: 0.00331729\n",
      "[600]\tvalid_0's rmse: 0.00232698\n",
      "[700]\tvalid_0's rmse: 0.0018306\n",
      "[800]\tvalid_0's rmse: 0.00153427\n",
      "[900]\tvalid_0's rmse: 0.00133749\n",
      "[1000]\tvalid_0's rmse: 0.00119666\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.00119666\n",
      "LightGBM Test RMSE: 0.0012\n",
      "Training memory-efficient SVD model with parallel processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD Training: 100%|██████████| 6/6 [00:00<00:00, 518.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models and data...\n",
      "Models and data ready. Starting recommendation system...\n",
      "\n",
      "Evaluating model contributions:\n",
      "Generating CF recommendations for movie_id: 155\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4401\n",
      "Number of recommendations generated: 7\n",
      "Generating CF recommendations for movie_id: 155\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 3958\n",
      "Number of recommendations generated: 15\n",
      "Content-based recs: 20\n",
      "CF recs: 15\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Model contributions for 'the dark knight':\n",
      "Content-based recommendations: Batman: Under the Red Hood, Batman Begins, The Dark Knight Rises, Batman: The Killing Joke, Batman Forever, Teenage Mutant Ninja Turtles, Batman, Batman Returns, Watchmen, Superman IV: The Quest for Peace\n",
      "Collaborative filtering (SVD) recommendations: Once Were Warriors, Three Colors: Red, Solaris, The Million Dollar Hotel, Men in Black II, Terminator 3: Rise of the Machines, Sissi\n",
      "Neural CF recommendations: Taxi Driver, Family Plot, Cool as Ice, Ice Age, Veronika Voss, Hero, Sympathy for Mr. Vengeance, The Mummy: Tomb of the Dragon Emperor, Not Here to Be Loved, Mama\n",
      "LightGBM recommendations: Heat, Scarface, Training Day, Man on Fire, The Fast and the Furious: Tokyo Drift, Fast & Furious, Drive, The Dark Knight Rises, Gangster Squad, Need for Speed\n",
      "Hybrid recommendations: The Dark Knight Rises, Batman Begins, Batman Returns, Taxi Driver, Batman: Under the Red Hood, Family Plot, A View to a Kill, Batman: The Killing Joke, American Beauty, Batman Forever\n",
      "\n",
      "Model contribution to hybrid recommendations:\n",
      "Content-based: 0.60\n",
      "Collaborative filtering (SVD): 0.00\n",
      "Neural CF: 0.20\n",
      "LightGBM: 0.10\n",
      "Generating CF recommendations for movie_id: 27205\n",
      "Number of sampled users: 5\n",
      "Number of considered movies: 4368\n",
      "Number of recommendations generated: 4\n",
      "Generating CF recommendations for movie_id: 27205\n",
      "Number of sampled users: 5\n",
      "Number of considered movies: 4368\n",
      "Number of recommendations generated: 9\n",
      "Content-based recs: 20\n",
      "CF recs: 9\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Model contributions for 'inception':\n",
      "Content-based recommendations: Minority Report, Last Embrace, Starcrash, The Anomaly, Salton Sea, Cyborg 3: The Recycler, Limitless, The Midnight Man, Unbreakable, Wanted\n",
      "Collaborative filtering (SVD) recommendations: Men in Black II, Terminator 3: Rise of the Machines, Monsieur  Hulot's Holiday, Bridge to Terabithia\n",
      "Neural CF recommendations: Noises Off..., Contact High, Charlie Countryman, In Fear, Katt Williams: Kattpacalypse, Why Man Creates, Mardock Scramble: The First Compression, Mods, Unbowed, Olé !\n",
      "LightGBM recommendations: X2, The Matrix Reloaded, Paycheck, Sky Captain and the World of Tomorrow, X-Men: The Last Stand, X-Men Origins: Wolverine, Oblivion, The Maze Runner, Jurassic World, Mad Max: Fury Road\n",
      "Hybrid recommendations: Minority Report, Last Embrace, Noises Off..., Mutant Aliens, The Anomaly, Starcrash, I Drink Your Blood, Harmful Insect, Cyborg 3: The Recycler, Salton Sea\n",
      "\n",
      "Model contribution to hybrid recommendations:\n",
      "Content-based: 0.60\n",
      "Collaborative filtering (SVD): 0.00\n",
      "Neural CF: 0.10\n",
      "LightGBM: 0.00\n",
      "Generating CF recommendations for movie_id: 680\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4939\n",
      "Number of recommendations generated: 7\n",
      "Generating CF recommendations for movie_id: 680\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4880\n",
      "Number of recommendations generated: 12\n",
      "Content-based recs: 20\n",
      "CF recs: 12\n",
      "NCF recs: 21\n",
      "LightGBM recs: 20\n",
      "\n",
      "Model contributions for 'pulp fiction':\n",
      "Content-based recommendations: Something Wild, Blonde Crazy, American Buffalo, Nice Dreams, Bad Santa 2, Death Race 2, The Ice Harvest, The Expendables 2, American Crude, Hitch Hike\n",
      "Collaborative filtering (SVD) recommendations: Sleepless in Seattle, The 39 Steps, Solaris, The Million Dollar Hotel, Scarface, Men in Black II, Terminator 3: Rise of the Machines\n",
      "Neural CF recommendations: Jackie Brown, The Color Purple, Born on the Fourth of July, The Wanderers, Bloody Sunday, The Jane Austen Book Club, Wood & Stock: Sexo, Orégano e Rock'n'Roll, Casanova Brown, A Matter of Time, Dinocroc\n",
      "LightGBM recommendations: Reservoir Dogs, Cape Fear, Snatch, Ocean's Eleven, Ocean's Twelve, Ocean's Thirteen, The Call, Now You See Me, Sin City: A Dame to Kill For, Legend\n",
      "Hybrid recommendations: Jackie Brown, Something Wild, The Ruling Class, Blonde Crazy, Aliens, Reservoir Dogs, Manhattan, American Buffalo, Nice Dreams, Bad Santa 2\n",
      "\n",
      "Model contribution to hybrid recommendations:\n",
      "Content-based: 0.50\n",
      "Collaborative filtering (SVD): 0.00\n",
      "Neural CF: 0.10\n",
      "LightGBM: 0.10\n",
      "Generating CF recommendations for movie_id: 278\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4575\n",
      "Number of recommendations generated: 7\n",
      "Generating CF recommendations for movie_id: 278\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 3664\n",
      "Number of recommendations generated: 17\n",
      "Content-based recs: 20\n",
      "CF recs: 17\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Model contributions for 'the shawshank redemption':\n",
      "Content-based recommendations: Felon, Cool Hand Luke, Fury, 20,000 Years in Sing Sing, Another 48 Hrs., Escape from Alcatraz, Cape Fear, Death Race, The Chase, Out of the Furnace\n",
      "Collaborative filtering (SVD) recommendations: Once Were Warriors, Solaris, The Million Dollar Hotel, 48 Hrs., The Hours, Terminator 3: Rise of the Machines, Sissi\n",
      "Neural CF recommendations: Tie Me Up! Tie Me Down!, Snake Eyes, Beneath the Planet of the Apes, Cesar and Rosalie, The Passion of Anna, The Swan, It Happened at the World's Fair, Mortadelo & Filemon: The Big Adventure, Bush Christmas, The Mountain Road\n",
      "LightGBM recommendations: Casino, Taxi Driver, Trainspotting, The Godfather, GoodFellas, The Godfather: Part II, Stand by Me, Requiem for a Dream, Catch Me If You Can, American Hustle\n",
      "Hybrid recommendations: Cool Hand Luke, Felon, Leaving Las Vegas, 20,000 Years in Sing Sing, Fury, Tie Me Up! Tie Me Down!, Milk Money, Escape from Alcatraz, Snake Eyes, Another 48 Hrs.\n",
      "\n",
      "Model contribution to hybrid recommendations:\n",
      "Content-based: 0.60\n",
      "Collaborative filtering (SVD): 0.00\n",
      "Neural CF: 0.20\n",
      "LightGBM: 0.00\n",
      "Generating CF recommendations for movie_id: 13\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4715\n",
      "Number of recommendations generated: 7\n",
      "Generating CF recommendations for movie_id: 13\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4933\n",
      "Number of recommendations generated: 13\n",
      "Content-based recs: 20\n",
      "CF recs: 13\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Model contributions for 'forrest gump':\n",
      "Content-based recommendations: The Adjustment Bureau, Me Before You, Me Before You, One Hour Photo, The Hours, He Was a Quiet Man, Sommersby, I Walk the Line, Lovers of the Arctic Circle, Frequency\n",
      "Collaborative filtering (SVD) recommendations: Three Colors: Red, Batman Returns, Solaris, The Million Dollar Hotel, Monsoon Wedding, Terminator 3: Rise of the Machines, Silent Hill\n",
      "Neural CF recommendations: Nick of Time, Cop Land, The Perfect Storm, Raw Deal, National Treasure, Manderlay, The Number 23, Conversations with My Gardener, James Dean, The Bogus Witch Project\n",
      "LightGBM recommendations: 10 Things I Hate About You, Love Actually, Hitch, The Devil Wears Prada, Juno, The Proposal, (500) Days of Summer, Crazy, Stupid, Love., Moonrise Kingdom, Silver Linings Playbook\n",
      "Hybrid recommendations: Me Before You, Nick of Time, The Adjustment Bureau, Cop Land, Rocky II, He Was a Quiet Man, The Hours, Licence to Kill, One Hour Photo, Annie Hall\n",
      "\n",
      "Model contribution to hybrid recommendations:\n",
      "Content-based: 0.50\n",
      "Collaborative filtering (SVD): 0.00\n",
      "Neural CF: 0.20\n",
      "LightGBM: 0.00\n",
      "\n",
      "Generating recommendations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CF recommendations for movie_id: 155\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4910\n",
      "Number of recommendations generated: 12\n",
      "Content-based recs: 20\n",
      "CF recs: 12\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 1/5 [01:24<05:38, 84.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'The Dark Knight':\n",
      "1. The Dark Knight Rises\n",
      "2. Batman Begins\n",
      "3. Taxi Driver\n",
      "4. Batman: Under the Red Hood\n",
      "5. Family Plot\n",
      "6. A View to a Kill\n",
      "7. Batman: The Killing Joke\n",
      "8. American Beauty\n",
      "9. Batman Forever\n",
      "10. Cool as Ice\n",
      "Generating CF recommendations for movie_id: 27205\n",
      "Number of sampled users: 5\n",
      "Number of considered movies: 4368\n",
      "Number of recommendations generated: 9\n",
      "Content-based recs: 20\n",
      "CF recs: 9\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|████      | 2/5 [02:26<03:34, 71.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'Inception':\n",
      "1. Minority Report\n",
      "2. Last Embrace\n",
      "3. Noises Off...\n",
      "4. Mutant Aliens\n",
      "5. The Anomaly\n",
      "6. Starcrash\n",
      "7. I Drink Your Blood\n",
      "8. Harmful Insect\n",
      "9. Cyborg 3: The Recycler\n",
      "10. Salton Sea\n",
      "Generating CF recommendations for movie_id: 680\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4928\n",
      "Number of recommendations generated: 11\n",
      "Content-based recs: 20\n",
      "CF recs: 11\n",
      "NCF recs: 21\n",
      "LightGBM recs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|██████    | 3/5 [03:52<02:36, 78.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'Pulp Fiction':\n",
      "1. Jackie Brown\n",
      "2. Something Wild\n",
      "3. The Ruling Class\n",
      "4. Blonde Crazy\n",
      "5. Aliens\n",
      "6. Reservoir Dogs\n",
      "7. Manhattan\n",
      "8. American Buffalo\n",
      "9. Nice Dreams\n",
      "10. Bad Santa 2\n",
      "Generating CF recommendations for movie_id: 278\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4590\n",
      "Number of recommendations generated: 15\n",
      "Content-based recs: 20\n",
      "CF recs: 15\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|████████  | 4/5 [05:19<01:21, 81.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'The Shawshank Redemption':\n",
      "1. Cool Hand Luke\n",
      "2. Felon\n",
      "3. Leaving Las Vegas\n",
      "4. 20,000 Years in Sing Sing\n",
      "5. Tie Me Up! Tie Me Down!\n",
      "6. Fury\n",
      "7. Milk Money\n",
      "8. Escape from Alcatraz\n",
      "9. Snake Eyes\n",
      "10. Another 48 Hrs.\n",
      "Generating CF recommendations for movie_id: 13\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4890\n",
      "Number of recommendations generated: 12\n",
      "Content-based recs: 20\n",
      "CF recs: 12\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [06:45<00:00, 81.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for 'Forrest Gump':\n",
      "1. Me Before You\n",
      "2. Nick of Time\n",
      "3. The Adjustment Bureau\n",
      "4. Cop Land\n",
      "5. Rocky II\n",
      "6. He Was a Quiet Man\n",
      "7. The Hours\n",
      "8. Licence to Kill\n",
      "9. One Hour Photo\n",
      "10. Annie Hall\n",
      "Generating CF recommendations for movie_id: 375315\n",
      "No users found who rated movie_id: 375315\n",
      "Content-based recs: 20\n",
      "CF recs: 0\n",
      "NCF recs: 0\n",
      "LightGBM recs: 20\n",
      "\n",
      "Recommendations for 'the salesman':\n",
      "1. Life+1 Day\n",
      "2. The Father\n",
      "3. Through the Olive Trees\n",
      "4. I Am Taraneh, I Am Fifteen Years Old\n",
      "5. Circumstance\n",
      "6. The Beat That My Heart Skipped\n",
      "7. Under the Skin of the City\n",
      "8. Black Ice\n",
      "9. About Elly\n",
      "10. The Apple\n",
      "Generating CF recommendations for movie_id: 155\n",
      "Number of sampled users: 100\n",
      "Number of considered movies: 4657\n",
      "Number of recommendations generated: 13\n",
      "Content-based recs: 20\n",
      "CF recs: 13\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Recommendations for 'the dark knight':\n",
      "1. The Dark Knight Rises\n",
      "2. Batman Begins\n",
      "3. Batman Returns\n",
      "4. Taxi Driver\n",
      "5. Batman: Under the Red Hood\n",
      "6. Family Plot\n",
      "7. A View to a Kill\n",
      "8. Batman: The Killing Joke\n",
      "9. American Beauty\n",
      "10. Batman Forever\n",
      "Generating CF recommendations for movie_id: 157336\n",
      "Number of sampled users: 4\n",
      "Number of considered movies: 1444\n",
      "Number of recommendations generated: 9\n",
      "Content-based recs: 20\n",
      "CF recs: 9\n",
      "NCF recs: 20\n",
      "LightGBM recs: 20\n",
      "\n",
      "Recommendations for 'interstellar':\n",
      "1. The Martian\n",
      "2. 2001: A Space Odyssey\n",
      "3. From Dusk Till Dawn\n",
      "4. While You Were Sleeping\n",
      "5. 20 Million Miles to Earth\n",
      "6. True Romance\n",
      "7. Alien: Covenant\n",
      "8. The Hitchhiker's Guide to the Galaxy\n",
      "9. Mimic\n",
      "10. Brief Encounter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import warnings\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def load_data(file_paths):\n",
    "    print(\"Loading datasets...\")\n",
    "    dfs = {}\n",
    "    for name, path in file_paths.items():\n",
    "        dfs[name] = pd.read_csv(path, low_memory=False)\n",
    "    return dfs\n",
    "\n",
    "def preprocess_data(dfs):\n",
    "    print(\"Preprocessing data...\")\n",
    "    movies_df = dfs['movies']\n",
    "    keywords_df = dfs['keywords']\n",
    "    credits_df = dfs['credits']\n",
    "    ratings_df = dfs['ratings']\n",
    "\n",
    "    # Rename 'id' to 'movieId' in movies_df\n",
    "    movies_df = movies_df.rename(columns={'id': 'movieId'})\n",
    "\n",
    "    # Merge datasets\n",
    "    for df in [keywords_df, credits_df]:\n",
    "        df['id'] = df['id'].astype(str)\n",
    "    movies_df['movieId'] = movies_df['movieId'].astype(str)\n",
    "\n",
    "    merged_df = movies_df.merge(keywords_df, left_on='movieId', right_on='id', how='left')\n",
    "    merged_df = merged_df.merge(credits_df, left_on='movieId', right_on='id', how='left')\n",
    "\n",
    "    # Clean and preprocess text data\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(text):\n",
    "        if isinstance(text, str):\n",
    "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "            tokens = word_tokenize(text)\n",
    "            return ' '.join([lemmatizer.lemmatize(word) for word in tokens if word not in stop_words])\n",
    "        return ''\n",
    "\n",
    "    for column in ['overview', 'keywords']:\n",
    "        tqdm.pandas(desc=f\"Cleaning {column}\")\n",
    "        merged_df[f'cleaned_{column}'] = merged_df[column].progress_apply(clean_text)\n",
    "\n",
    "    # Process other columns\n",
    "    merged_df['release_date'] = pd.to_datetime(merged_df['release_date'], errors='coerce')\n",
    "    merged_df['release_year'] = merged_df['release_date'].dt.year\n",
    "\n",
    "    merged_df['genres'] = merged_df['genres'].fillna('[]').apply(eval).apply(lambda x: [i['name'] for i in x] if x else [])\n",
    "    merged_df['genres_str'] = merged_df['genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    merged_df['cast'] = merged_df['cast'].fillna('[]').apply(eval).apply(lambda x: [i['name'] for i in x[:5]] if x else [])\n",
    "    merged_df['cast_str'] = merged_df['cast'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    merged_df['crew'] = merged_df['crew'].fillna('[]').apply(eval)\n",
    "    merged_df['director'] = merged_df['crew'].apply(lambda x: ' '.join([i['name'] for i in x if i['job'] == 'Director']))\n",
    "\n",
    "    merged_df['combined_features'] = (\n",
    "        merged_df['cleaned_overview'] + ' ' +\n",
    "        merged_df['cleaned_keywords'] + ' ' +\n",
    "        merged_df['genres_str'] + ' ' +\n",
    "        merged_df['cast_str'] + ' ' +\n",
    "        merged_df['director']\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def engineer_features(df):\n",
    "    print(\"Engineering features...\")\n",
    "    # Ensure that numerical columns are numeric\n",
    "    for col in ['popularity', 'vote_average', 'vote_count', 'release_year']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Fill NaNs with median values\n",
    "    for col in ['popularity', 'vote_average', 'vote_count', 'release_year']:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    scaler = MinMaxScaler()\n",
    "    df[['popularity', 'vote_average', 'vote_count', 'release_year']] = scaler.fit_transform(\n",
    "        df[['popularity', 'vote_average', 'vote_count', 'release_year']]\n",
    "    )\n",
    "\n",
    "    # Encode categorical features\n",
    "    le = LabelEncoder()\n",
    "    for col in ['adult', 'status', 'original_language']:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    # Create genre one-hot encoding\n",
    "    genres = set()\n",
    "    for genre_list in df['genres']:\n",
    "        genres.update(genre_list)\n",
    "    for genre in genres:\n",
    "        df[f'genre_{genre}'] = df['genres'].apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "class DistilBERTEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def encode(self, texts, batch_size=32):\n",
    "        encodings = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding texts\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            encodings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
    "        return np.vstack(encodings)\n",
    "\n",
    "def content_based_recommendations(movie_title, df, encodings, top_n=10):\n",
    "    # Normalize the input movie title\n",
    "    movie_title = movie_title.strip().lower()\n",
    "\n",
    "    # Normalize the titles in the dataframe\n",
    "    df['normalized_title'] = df['title'].str.strip().str.lower()\n",
    "\n",
    "    movie_index = df[df['normalized_title'] == movie_title].index\n",
    "    if len(movie_index) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found in the dataset.\")\n",
    "        return []\n",
    "    idx = movie_index[0]\n",
    "    movie_encoding = encodings[idx].reshape(1, -1)\n",
    "    cosine_sim = cosine_similarity(movie_encoding, encodings).flatten()\n",
    "    sim_scores = list(enumerate(cosine_sim))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return df['title'].iloc[movie_indices].tolist()\n",
    "\n",
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size=64, layers=[ 128, 64, 32]):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_size = embedding_size * 2\n",
    "        for layer_size in layers:\n",
    "            self.fc_layers.append(nn.Linear(input_size, layer_size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "            self.fc_layers.append(nn.BatchNorm1d(layer_size))\n",
    "            self.fc_layers.append(nn.Dropout(0.2))\n",
    "            input_size = layer_size\n",
    "\n",
    "        self.output_layer = nn.Linear(layers[-1], 1)\n",
    "\n",
    "    def forward(self, user_input, movie_input):\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        movie_embedded = self.movie_embedding(movie_input)\n",
    "\n",
    "        vector = torch.cat([user_embedded, movie_embedded], dim=-1)\n",
    "        for layer in self.fc_layers:\n",
    "            vector = layer(vector)\n",
    "\n",
    "        output = self.output_layer(vector)\n",
    "        return output.squeeze()\n",
    "\n",
    "class MovieRatingDataset(Dataset):\n",
    "    def __init__(self, user_ids, movie_ids, ratings):\n",
    "        self.user_ids = user_ids\n",
    "        self.movie_ids = movie_ids\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.movie_ids[idx], self.ratings[idx]\n",
    "\n",
    "def train_ncf_model(ratings_df, epochs=25, batch_size=1024):\n",
    "    print(\"Training NCF model...\")\n",
    "    user_encoder = LabelEncoder()\n",
    "    movie_encoder = LabelEncoder()\n",
    "\n",
    "    user_ids = user_encoder.fit_transform(ratings_df['userId'])\n",
    "    movie_ids = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "    ratings = ratings_df['rating'].values\n",
    "\n",
    "    train_user_ids, val_user_ids, train_movie_ids, val_movie_ids, train_ratings, val_ratings = train_test_split(\n",
    "        user_ids, movie_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = MovieRatingDataset(train_user_ids, train_movie_ids, train_ratings)\n",
    "    val_dataset = MovieRatingDataset(val_user_ids, val_movie_ids, val_ratings)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_movies = len(movie_encoder.classes_)\n",
    "    model = NCF(num_users, num_movies)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for user_input, movie_input, rating in train_dataloader:\n",
    "            user_input, movie_input, rating = user_input.to(device), movie_input.to(device), rating.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(user_input, movie_input)\n",
    "            loss = criterion(output, rating)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for user_input, movie_input, rating in val_dataloader:\n",
    "                user_input, movie_input, rating = user_input.to(device), movie_input.to(device), rating.float().to(device)\n",
    "                output = model(user_input, movie_input)\n",
    "                loss = criterion(output, rating)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_ncf_model.pth')\n",
    "\n",
    "    model.load_state_dict(torch.load('best_ncf_model.pth'))\n",
    "    return model, user_encoder, movie_encoder\n",
    "\n",
    "def train_lightgbm_model(df):\n",
    "    print(\"Training LightGBM model...\")\n",
    "\n",
    "    features = ['popularity', 'vote_average', 'vote_count', 'release_year', 'adult', 'status', 'original_language'] + \\\n",
    "               [col for col in df.columns if col.startswith('genre_')]\n",
    "    X = df[features]\n",
    "    y = df['vote_average']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"LightGBM Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    joblib.dump(model, 'lightgbm_model.joblib')\n",
    "    return model\n",
    "\n",
    "\n",
    "class MemoryEfficientSVDModel:\n",
    "    def __init__(self, user_factors, movie_factors, user_encoder, movie_encoder):\n",
    "        self.user_factors = user_factors\n",
    "        self.movie_factors = movie_factors\n",
    "        self.user_encoder = user_encoder\n",
    "        self.movie_encoder = movie_encoder\n",
    "\n",
    "    def predict(self, user, movie):\n",
    "        try:\n",
    "            user_idx = self.user_encoder.transform([str(user)])[0]\n",
    "            movie_idx = self.movie_encoder.transform([str(movie)])[0]\n",
    "            prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "            return min(max(prediction, 0.5), 5)  # Clip prediction between 0.5 and 5\n",
    "        except ValueError:\n",
    "            return 2.5  # Return average rating if user or movie is not in the training set\n",
    "\n",
    "    def save(self, path):\n",
    "        np.save(f\"{path}_user_factors.npy\", self.user_factors)\n",
    "        np.save(f\"{path}_movie_factors.npy\", self.movie_factors)\n",
    "        joblib.dump(self.user_encoder, f\"{path}_user_encoder.joblib\")\n",
    "        joblib.dump(self.movie_encoder, f\"{path}_movie_encoder.joblib\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        user_factors = np.load(f\"{path}_user_factors.npy\")\n",
    "        movie_factors = np.load(f\"{path}_movie_factors.npy\")\n",
    "        user_encoder = joblib.load(f\"{path}_user_encoder.joblib\")\n",
    "        movie_encoder = joblib.load(f\"{path}_movie_encoder.joblib\")\n",
    "        return cls(user_factors, movie_factors, user_encoder, movie_encoder)\n",
    "\n",
    "def train_svd_model(ratings_df, n_factors=100, n_iter=20, batch_size=50000, n_jobs=-1):\n",
    "    print(\"Training memory-efficient SVD model with parallel processing...\")\n",
    "\n",
    "    # Ensure movieId and userId are strings\n",
    "    ratings_df['movieId'] = ratings_df['movieId'].astype(str)\n",
    "    ratings_df['userId'] = ratings_df['userId'].astype(str)\n",
    "\n",
    "    # Encode user and movie IDs\n",
    "    user_encoder = LabelEncoder()\n",
    "    movie_encoder = LabelEncoder()\n",
    "\n",
    "    user_ids = user_encoder.fit_transform(ratings_df['userId'])\n",
    "    movie_ids = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "\n",
    "    # Convert to CSR matrix\n",
    "    ratings_sparse = csr_matrix((ratings_df['rating'], (user_ids, movie_ids)))\n",
    "\n",
    "    n_users, n_movies = ratings_sparse.shape\n",
    "\n",
    "    # Initialize factor matrices\n",
    "    user_factors = np.zeros((n_users, n_factors))\n",
    "    movie_factors = np.zeros((n_movies, n_factors))\n",
    "\n",
    "    def process_batch(start, end):\n",
    "        batch = ratings_sparse[start:end, :]\n",
    "        U, S, Vt = randomized_svd(batch, n_components=n_factors, n_iter=n_iter, random_state=42)\n",
    "        return U, S, Vt\n",
    "\n",
    "    # Process data in batches using parallel processing\n",
    "    total_batches = (n_users + batch_size - 1) // batch_size\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_batch)(start, min(start + batch_size, n_users))\n",
    "        for start in tqdm(range(0, n_users, batch_size), total=total_batches, desc=\"SVD Training\")\n",
    "    )\n",
    "\n",
    "    # Aggregate results\n",
    "    for i, (U, S, Vt) in enumerate(results):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, n_users)\n",
    "        user_factors[start:end] = U * np.sqrt(S)\n",
    "        movie_factors += Vt.T * np.sqrt(S)\n",
    "\n",
    "    # Normalize factor matrices\n",
    "    user_factors /= len(results)\n",
    "    movie_factors /= len(results)\n",
    "\n",
    "    model = MemoryEfficientSVDModel(user_factors, movie_factors, user_encoder, movie_encoder)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_cf_recommendations(movie_id, df, ratings_df, svd_model, top_n=20, n_users=100, n_movies=5000):\n",
    "    print(f\"Generating CF recommendations for movie_id: {movie_id}\")\n",
    "\n",
    "    movie_id = str(movie_id)\n",
    "    ratings_df['movieId'] = ratings_df['movieId'].astype(str)\n",
    "    df['movieId'] = df['movieId'].astype(str)\n",
    "\n",
    "    users_who_rated = ratings_df[ratings_df['movieId'] == movie_id]['userId'].unique()\n",
    "    \n",
    "    if len(users_who_rated) == 0:\n",
    "        print(f\"No users found who rated movie_id: {movie_id}\")\n",
    "        return []  # Return an empty list if no users rated this movie\n",
    "\n",
    "    sampled_users = np.random.choice(users_who_rated, min(n_users, len(users_who_rated)), replace=False)\n",
    "    print(f\"Number of sampled users: {len(sampled_users)}\")\n",
    "\n",
    "    if len(sampled_users) == 0:\n",
    "        print(f\"No sampled users for movie_id: {movie_id}\")\n",
    "        return []  # Return an empty list if no users were sampled\n",
    "\n",
    "    movie_counts = ratings_df['movieId'].value_counts()\n",
    "    top_movies = movie_counts.nlargest(n_movies).index\n",
    "    movies_rated_by_users = ratings_df[(ratings_df['userId'].isin(sampled_users)) & (ratings_df['movieId'].isin(top_movies))]['movieId'].unique()\n",
    "    print(f\"Number of considered movies: {len(movies_rated_by_users)}\")\n",
    "\n",
    "    if len(movies_rated_by_users) == 0:\n",
    "        print(f\"No movies found rated by sampled users for movie_id: {movie_id}\")\n",
    "        return []  # Return an empty list if no movies were found\n",
    "\n",
    "    try:\n",
    "        user_factors = np.array([svd_model.user_factors[svd_model.user_encoder.transform([user])[0]] for user in sampled_users])\n",
    "        movie_factors = svd_model.movie_factors[svd_model.movie_encoder.transform(movies_rated_by_users)]\n",
    "\n",
    "        predictions = np.dot(user_factors, movie_factors.T)\n",
    "        avg_ratings = np.mean(predictions, axis=0)\n",
    "\n",
    "        movie_avg_ratings = list(zip(movies_rated_by_users, avg_ratings))\n",
    "        movie_avg_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_movie_ids = [movie for movie, _ in movie_avg_ratings[:top_n] if movie != movie_id]\n",
    "        recommendations = df[df['movieId'].isin(top_movie_ids)]['title'].tolist()\n",
    "        print(f\"Number of recommendations generated: {len(recommendations)}\")\n",
    "        return recommendations\n",
    "    except Exception as e:\n",
    "        print(f\"Error in CF recommendations: {str(e)}\")\n",
    "        return []  # Return an empty list if any error occurs\n",
    "\n",
    "def generate_ncf_recommendations(movie_id, df, ncf_model, movie_encoder, top_n=20):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ncf_model.eval()\n",
    "\n",
    "    try:\n",
    "        encoded_movie_id = movie_encoder.transform([movie_id])[0]\n",
    "    except ValueError:\n",
    "        return []\n",
    "\n",
    "    movie_input = torch.tensor([encoded_movie_id]).to(device)\n",
    "\n",
    "    similar_movies = []\n",
    "    for other_movie in df['movieId'].unique():\n",
    "        if other_movie != movie_id:\n",
    "            try:\n",
    "                encoded_other_movie = movie_encoder.transform([other_movie])[0]\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            other_movie_input = torch.tensor([encoded_other_movie]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                similarity = torch.cosine_similarity(\n",
    "                    ncf_model.movie_embedding(movie_input),\n",
    "                    ncf_model.movie_embedding(other_movie_input)\n",
    "                ).item()\n",
    "\n",
    "            similar_movies.append((other_movie, similarity))\n",
    "\n",
    "    similar_movies.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_movie_ids = [movie_id for movie_id, _ in similar_movies[:top_n]]\n",
    "    return df[df['movieId'].isin(top_movie_ids)]['title'].tolist()\n",
    "\n",
    "def generate_lgbm_recommendations(movie_id, df, lgbm_model, top_n=20):\n",
    "    features = ['popularity', 'vote_average', 'vote_count', 'release_year', 'adult', 'status', 'original_language'] + \\\n",
    "               [col for col in df.columns if col.startswith('genre_')]\n",
    "\n",
    "    input_movie_features = df[df['movieId'] == movie_id][features].values\n",
    "\n",
    "    similarities = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['movieId'] != movie_id:\n",
    "            other_movie_features = row[features].values\n",
    "            similarity = cosine_similarity(input_movie_features, other_movie_features.reshape(1, -1))[0][0]\n",
    "            similarities.append((row['movieId'], similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_movie_ids = [movie_id for movie_id, _ in similarities[:top_n]]\n",
    "    return df[df['movieId'].isin(top_movie_ids)]['title'].tolist()\n",
    "\n",
    "def hybrid_recommendations(movie_title, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model, top_n=10):\n",
    "    movie_title = movie_title.strip().lower()\n",
    "    df['normalized_title'] = df['title'].str.strip().str.lower()\n",
    "\n",
    "    if movie_title not in df['normalized_title'].values:\n",
    "        raise ValueError(f\"Movie '{movie_title}' not found in the dataset.\")\n",
    "\n",
    "    movie_id = df[df['normalized_title'] == movie_title]['movieId'].iloc[0]\n",
    "\n",
    "    content_recs = content_based_recommendations(movie_title, df, encodings, top_n=top_n*2)\n",
    "    cf_recs = generate_cf_recommendations(movie_id, df, ratings_df, svd_model, top_n=top_n*2)\n",
    "    ncf_recs = generate_ncf_recommendations(movie_id, df, ncf_model, movie_encoder, top_n=top_n*2)\n",
    "    lgbm_recs = generate_lgbm_recommendations(movie_id, df, lgbm_model, top_n=top_n*2)\n",
    "\n",
    "    print(f\"Content-based recs: {len(content_recs)}\")\n",
    "    print(f\"CF recs: {len(cf_recs)}\")\n",
    "    print(f\"NCF recs: {len(ncf_recs)}\")\n",
    "    print(f\"LightGBM recs: {len(lgbm_recs)}\")\n",
    "\n",
    "    # Get genre and director info for the input movie\n",
    "    input_movie = df[df['normalized_title'] == movie_title].iloc[0]\n",
    "    input_movie_genres = set(input_movie['genres'])\n",
    "    \n",
    "    input_movie_director = input_movie['director']\n",
    "\n",
    "    hybrid_recs = []\n",
    "    content_weight = 0.3\n",
    "    cf_weight = 0.2\n",
    "    ncf_weight = 0.3\n",
    "    lgbm_weight = 0.2\n",
    "    genre_weight = 0.05\n",
    "    director_weight = 0.05\n",
    "\n",
    "    all_movies = set(content_recs + cf_recs + ncf_recs + lgbm_recs)\n",
    "    hybrid_scores = {}\n",
    "\n",
    "    for movie in all_movies:\n",
    "        if movie != movie_title:\n",
    "            content_score = content_weight * (1 - content_recs.index(movie) / len(content_recs)) if movie in content_recs else 0\n",
    "            cf_score = cf_weight * (1 - cf_recs.index(movie) / len(cf_recs)) if movie in cf_recs else 0\n",
    "            ncf_score = ncf_weight * (1 - ncf_recs.index(movie) / len(ncf_recs)) if movie in ncf_recs else 0\n",
    "            lgbm_score = lgbm_weight * (1 - lgbm_recs.index(movie) / len(lgbm_recs)) if movie in lgbm_recs else 0\n",
    "\n",
    "            # Calculate genre similarity\n",
    "            movie_genres = set(df[df['title'] == movie]['genres'].iloc[0])\n",
    "            genre_similarity = len(input_movie_genres.intersection(movie_genres)) / len(input_movie_genres.union(movie_genres))\n",
    "            genre_score = genre_weight * genre_similarity\n",
    "\n",
    "            # Calculate director similarity\n",
    "            movie_director = df[df['title'] == movie]['director'].iloc[0]\n",
    "            director_score = director_weight if input_movie_director == movie_director else 0\n",
    "\n",
    "            # Combine scores\n",
    "            hybrid_scores[movie] = content_score + cf_score + ncf_score + lgbm_score + genre_score + director_score\n",
    "\n",
    "    sorted_recs = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    hybrid_recs = [movie for movie, score in sorted_recs[:top_n]]\n",
    "\n",
    "    return hybrid_recs\n",
    "\n",
    "def evaluate_model_contributions(movie_title, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model):\n",
    "    # Normalize movie_title\n",
    "    movie_title = movie_title.strip().lower()\n",
    "\n",
    "    # Normalize titles in DataFrame for better matching\n",
    "    df['normalized_title'] = df['title'].str.strip().str.lower()\n",
    "\n",
    "    # Check if the movie exists in the DataFrame\n",
    "    if movie_title not in df['normalized_title'].values:\n",
    "        raise ValueError(f\"Movie '{movie_title}' not found in the dataset.\")\n",
    "\n",
    "    # Get the movie_id for the given movie_title\n",
    "    movie_id = df[df['normalized_title'] == movie_title]['movieId'].iloc[0]\n",
    "\n",
    "    content_recs = content_based_recommendations(movie_title, df, encodings, top_n=10)\n",
    "    cf_recs = generate_cf_recommendations(movie_id,df, ratings_df, svd_model, top_n=10)\n",
    "    ncf_recs = generate_ncf_recommendations(movie_id, df, ncf_model, movie_encoder, top_n=10)\n",
    "    lgbm_recs = generate_lgbm_recommendations(movie_id, df, lgbm_model, top_n=10)\n",
    "\n",
    "    hybrid_recs = hybrid_recommendations(movie_title, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model)\n",
    "\n",
    "    print(f\"\\nModel contributions for '{movie_title}':\")\n",
    "    print(f\"Content-based recommendations: {', '.join(content_recs)}\")\n",
    "    print(f\"Collaborative filtering (SVD) recommendations: {', '.join(cf_recs)}\")\n",
    "    print(f\"Neural CF recommendations: {', '.join(ncf_recs)}\")\n",
    "    print(f\"LightGBM recommendations: {', '.join(lgbm_recs)}\")\n",
    "    print(f\"Hybrid recommendations: {', '.join(hybrid_recs)}\")\n",
    "\n",
    "    # Calculate overlap between hybrid and individual models\n",
    "    content_overlap = len(set(hybrid_recs) & set(content_recs)) / len(hybrid_recs)\n",
    "    cf_overlap = len(set(hybrid_recs) & set(cf_recs)) / len(hybrid_recs)\n",
    "    ncf_overlap = len(set(hybrid_recs) & set(ncf_recs)) / len(hybrid_recs)\n",
    "    lgbm_overlap = len(set(hybrid_recs) & set(lgbm_recs)) / len(hybrid_recs)\n",
    "\n",
    "    print(\"\\nModel contribution to hybrid recommendations:\")\n",
    "    print(f\"Content-based: {content_overlap:.2f}\")\n",
    "    print(f\"Collaborative filtering (SVD): {cf_overlap:.2f}\")\n",
    "    print(f\"Neural CF: {ncf_overlap:.2f}\")\n",
    "    print(f\"LightGBM: {lgbm_overlap:.2f}\")\n",
    "\n",
    "def main():\n",
    "    file_paths = {\n",
    "        'keywords': 'path\\\\keywords.csv',\n",
    "        'credits': 'path\\\\credits.csv',\n",
    "        'ratings': 'path\\\\ratings.csv',\n",
    "        'movies': 'path\\\\movies_metadata.csv'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Check if saved models and data exist\n",
    "        if os.path.exists('preprocessed_df.pkl') and os.path.exists('ratings_df.pkl') and \\\n",
    "           os.path.exists('encodings.npy') and os.path.exists('ncf_model.pth') and \\\n",
    "           os.path.exists('user_encoder.joblib') and os.path.exists('movie_encoder.joblib') and \\\n",
    "           os.path.exists('lgbm_model.joblib') and os.path.exists('svd_model'):\n",
    "            \n",
    "            print(\"Loading saved models and data...\")\n",
    "            df = pd.read_pickle('preprocessed_df.pkl')\n",
    "            ratings_df = pd.read_pickle('ratings_df.pkl')\n",
    "            encodings = np.load('encodings.npy')\n",
    "            \n",
    "            ncf_model = NCF(len(np.unique(ratings_df['userId'])), len(np.unique(ratings_df['movieId'])))\n",
    "            ncf_model.load_state_dict(torch.load('ncf_model.pth'))\n",
    "            user_encoder = joblib.load('user_encoder.joblib')\n",
    "            movie_encoder = joblib.load('movie_encoder.joblib')\n",
    "            \n",
    "            lgbm_model = joblib.load('lgbm_model.joblib')\n",
    "            \n",
    "            svd_model = MemoryEfficientSVDModel.load('svd_model')\n",
    "        else:\n",
    "            print(\"Processing data and training models...\")\n",
    "            dfs = load_data(file_paths)\n",
    "            gc.collect()\n",
    "            merged_df = preprocess_data(dfs)\n",
    "            gc.collect()\n",
    "            df = engineer_features(merged_df)\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"Encoding movie features with DistilBERT...\")\n",
    "            distilbert_encoder = DistilBERTEncoder()\n",
    "            gc.collect()\n",
    "            encodings = distilbert_encoder.encode(df['combined_features'].tolist())\n",
    "\n",
    "            ratings_df = dfs['ratings']\n",
    "            gc.collect()\n",
    "\n",
    "            ncf_model, user_encoder, movie_encoder = train_ncf_model(ratings_df)\n",
    "            gc.collect()\n",
    "            lgbm_model = train_lightgbm_model(df, ratings_df)\n",
    "            gc.collect()\n",
    "            svd_model = train_svd_model(ratings_df)\n",
    "            gc.collect()\n",
    "\n",
    "            # Save all models and data\n",
    "            print(\"Saving models and data...\")\n",
    "            df.to_pickle('preprocessed_df.pkl')\n",
    "            ratings_df.to_pickle('ratings_df.pkl')\n",
    "            np.save('encodings.npy', encodings)\n",
    "            \n",
    "            torch.save(ncf_model.state_dict(), 'ncf_model.pth')\n",
    "            joblib.dump(user_encoder, 'user_encoder.joblib')\n",
    "            joblib.dump(movie_encoder, 'movie_encoder.joblib')\n",
    "            \n",
    "            joblib.dump(lgbm_model, 'lgbm_model.joblib')\n",
    "            \n",
    "            svd_model.save('svd_model')\n",
    "\n",
    "        print(\"Models and data ready. Starting recommendation system...\")\n",
    "\n",
    "        test_movies = [\n",
    "            \"The Dark Knight\",\n",
    "            \"Inception\",\n",
    "            \"Pulp Fiction\",\n",
    "            \"The Shawshank Redemption\",\n",
    "            \"Forrest Gump\"\n",
    "        ]\n",
    "\n",
    "        print(\"\\nEvaluating model contributions:\")\n",
    "        for movie in test_movies:\n",
    "            try:\n",
    "                evaluate_model_contributions(movie, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating '{movie}': {str(e)}\")\n",
    "\n",
    "        print(\"\\nGenerating recommendations:\")\n",
    "        for movie in tqdm(test_movies, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                recs = hybrid_recommendations(movie, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model)\n",
    "                print(f\"\\nRecommendations for '{movie}':\")\n",
    "                for i, rec in enumerate(recs, 1):\n",
    "                    print(f\"{i}. {rec}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating recommendations for '{movie}': {str(e)}\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nEnter a movie title (or 'quit' to exit): \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                recs = hybrid_recommendations(user_input, df, ratings_df, encodings, ncf_model, movie_encoder, lgbm_model, svd_model)\n",
    "                print(f\"\\nRecommendations for '{user_input}':\")\n",
    "                for i, rec in enumerate(recs, 1):\n",
    "                    print(f\"{i}. {rec}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
